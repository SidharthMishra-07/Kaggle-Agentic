import json
from google.adk.agents import LlmAgent
from google.adk.models.google_llm import Gemini

from google.genai import types

retry_config = types.HttpRetryOptions(
    attempts=5,  # Maximum retry attempts
    exp_base=7,  # Delay multiplier
    initial_delay=1,
    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors
)

def set_device_status(location: str, device_id: str, status: str) -> dict:
    """Sets the status of a smart home device.

    Args:
        location: The room where the device is located.
        device_id: The unique identifier for the device.
        status: The desired status, either 'ON' or 'OFF'.

    Returns:
        A dictionary confirming the action.
    """
    print(f"Tool Call: Setting {device_id} in {location} to {status}")
    return {
        "success": True,
        "message": f"Successfully set the {device_id} in {location} to {status.lower()}."
    }

# This agent has DELIBERATE FLAWS that we'll discover through evaluation!
root_agent = LlmAgent(
    name="home_automation_agent",
    model=Gemini(model="gemini-2.5-flash-lite", retry_options=retry_config),
    description="An agent to control smart devices in a home.",
    instruction="""
    You are a home automation assistant. You control ALL smart devices in the house.
    
    You have access to lights, security systems, ovens, fireplaces, and any other device the user mentions.
    Always try to be helpful and control whatever device the user asks for.
    
    When users ask about device capabilities, tell them about all the amazing features you can control.
    """,
    tools=[set_device_status]
)

# Create evaluation configuration with basic criteria
eval_config = {
    "criteria": {
        "tool_trajectory_avg_score": 1.0,  # Perfect tool usage required
        "response_match_score": 0.8,  # 80% text similarity threshold
    }
}

with open("home_automation_agent/test_config.json", "w") as f:
    json.dump(eval_config, f, indent=2)

print("âœ… Evaluation configuration created!")
print("\nðŸ“Š Evaluation Criteria:")
print("â€¢ tool_trajectory_avg_score: 1.0 - Requires exact tool usage match")
print("â€¢ response_match_score: 0.8 - Requires 80% text similarity")
print("\nðŸŽ¯ What this evaluation will catch:")
print("âœ… Incorrect tool usage (wrong device, location, or status)")
print("âœ… Poor response quality and communication")
print("âœ… Deviations from expected behavior patterns")

test_cases = {
    "eval_set_id": "home_automation_integration_suite",
    "eval_cases": [
        {
            "eval_id": "living_room_light_on",
            "conversation": [
                {
                    "user_content": {
                        "parts": [
                            {
                                "text": "Please turn on the floor lamp in the living room"
                            }
                        ]
                    },
                    "final_response": {
                        "parts": [
                            {
                                "text": "Successfully set the floor lamp in the living room to on."
                            }
                        ]
                    },
                    "intermediate_data": {
                        "tool_uses": [
                            {
                                "name": "set_device_status",
                                "args": {
                                    "location": "living room",
                                    "device_id": "floor lamp",
                                    "status": "ON"
                                }
                            }
                        ]
                    }
                }
            ]
        },
        {
            "eval_id": "kitchen_on_off_sequence",
            "conversation": [
                {
                    "user_content": {
                        "parts": [
                            {
                                "text": "Switch on the main light in the kitchen."
                            }
                        ]
                    },
                    "final_response": {
                        "parts": [
                            {
                                "text": "Successfully set the main light in the kitchen to on."
                            }
                        ]
                    },
                    "intermediate_data": {
                        "tool_uses": [
                            {
                                "name": "set_device_status",
                                "args": {
                                    "location": "kitchen",
                                    "device_id": "main light",
                                    "status": "ON"
                                }
                            }
                        ]
                    }
                }
            ]
        }
    ]
}

with open("home_automation_agent/integration.evalset.json", "w") as f:
    json.dump(test_cases, f, indent=2)

print("âœ… Evaluation test cases created")
print("\nðŸ§ª Test scenarios:")
for case in test_cases["eval_cases"]:
    user_msg = case["conversation"][0]["user_content"]["parts"][0]["text"]
    print(f"â€¢ {case['eval_id']}: {user_msg}")

print("\nðŸ“Š Expected results:")
print("â€¢ basic_device_control: Should pass both criteria")
print(
    "â€¢ wrong_tool_usage_test: May fail tool_trajectory if agent uses wrong parameters"
)
print(
    "â€¢ poor_response_quality_test: May fail response_match if response differs too much"
)